# Logstash configuration for KlikkFlow application logs

input {
  # Beats input (Filebeat)
  beats {
    port => 5044
    type => "beats"
  }

  # TCP input for application logs
  tcp {
    port => 5000
    type => "tcp"
    codec => json_lines
  }

  # GELF input for Docker logs
  gelf {
    port => 12201
    type => "gelf"
  }

  # HTTP input for webhook-style log ingestion
  http {
    port => 8080
    type => "http"
  }

  # Syslog input
  syslog {
    port => 5514
    type => "syslog"
  }
}

filter {
  # Add common fields
  mutate {
    add_field => { "[@metadata][index]" => "klikkflow-logs-%{+YYYY.MM.dd}" }
    add_field => { "[@metadata][document_type]" => "log" }
  }

  # Process different log types
  if [type] == "beats" {
    # Filebeat processing
    if [agent][type] == "filebeat" {
      # Parse container logs
      if [container][name] {
        mutate {
          add_field => { "service_name" => "%{[container][name]}" }
          add_field => { "log_source" => "container" }
        }

        # Parse Docker JSON logs
        if [message] =~ /^\{.*\}$/ {
          json {
            source => "message"
            target => "json_log"
          }

          if [json_log][level] {
            mutate {
              add_field => { "log_level" => "%{[json_log][level]}" }
            }
          }

          if [json_log][timestamp] {
            date {
              match => [ "[json_log][timestamp]", "ISO8601" ]
            }
          }

          if [json_log][service] {
            mutate {
              replace => { "service_name" => "%{[json_log][service]}" }
            }
          }
        }
      }

      # Parse Nginx access logs
      if [log][file][path] =~ /nginx.*access/ {
        grok {
          match => {
            "message" => '%{COMBINEDAPACHELOG} "%{WORD:request_method} %{URIPATH:request_path}(?:%{URIPARAM:request_params})? HTTP/%{NUMBER:http_version}" %{INT:response_code} %{INT:response_bytes} "%{DATA:referrer}" "%{DATA:user_agent}" rt=%{NUMBER:response_time}'
          }
        }

        mutate {
          add_field => { "log_type" => "nginx_access" }
          convert => { "response_code" => "integer" }
          convert => { "response_bytes" => "integer" }
          convert => { "response_time" => "float" }
        }

        if [response_code] >= 400 {
          mutate {
            add_field => { "log_level" => "error" }
          }
        } else {
          mutate {
            add_field => { "log_level" => "info" }
          }
        }
      }

      # Parse Nginx error logs
      if [log][file][path] =~ /nginx.*error/ {
        grok {
          match => {
            "message" => '(?<timestamp>%{YEAR}[./-]%{MONTHNUM}[./-]%{MONTHDAY}[- ]%{TIME}) \[%{LOGLEVEL:log_level}\] %{POSINT:pid}#%{NUMBER:tid}: (\*%{NUMBER:connection_id} )?%{GREEDYDATA:error_message}(, client: %{IPORHOST:client_ip})?(, server: %{IPORHOST:server})?(, request: "%{WORD:request_method} %{URIPATHPARAM:request_uri} HTTP/%{NUMBER:http_version}")?(, upstream: "%{URI:upstream}")?(, host: "%{IPORHOST:host}")?(, referrer: "%{URI:referrer}")?'
          }
        }

        mutate {
          add_field => { "log_type" => "nginx_error" }
        }

        date {
          match => [ "timestamp", "yyyy/MM/dd HH:mm:ss" ]
        }
      }
    }
  }

  # Process KlikkFlow application logs
  if [service_name] =~ /klikkflow/ {
    # Parse JSON structured logs
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "app_log"
      }

      # Extract common fields
      if [app_log][timestamp] {
        date {
          match => [ "[app_log][timestamp]", "ISO8601" ]
        }
      }

      if [app_log][level] {
        mutate {
          add_field => { "log_level" => "%{[app_log][level]}" }
        }
      }

      if [app_log][service] {
        mutate {
          replace => { "service_name" => "%{[app_log][service]}" }
        }
      }

      # Extract tracing information
      if [app_log][trace_id] {
        mutate {
          add_field => { "trace_id" => "%{[app_log][trace_id]}" }
        }
      }

      if [app_log][span_id] {
        mutate {
          add_field => { "span_id" => "%{[app_log][span_id]}" }
        }
      }

      # Extract workflow information
      if [app_log][workflow_id] {
        mutate {
          add_field => { "workflow_id" => "%{[app_log][workflow_id]}" }
        }
      }

      if [app_log][execution_id] {
        mutate {
          add_field => { "execution_id" => "%{[app_log][execution_id]}" }
        }
      }

      if [app_log][node_id] {
        mutate {
          add_field => { "node_id" => "%{[app_log][node_id]}" }
        }
      }

      # Extract error information
      if [app_log][error] {
        mutate {
          add_field => { "error_message" => "%{[app_log][error][message]}" }
          add_field => { "error_stack" => "%{[app_log][error][stack]}" }
        }
      }

      # Extract HTTP request information
      if [app_log][req] {
        mutate {
          add_field => { "http_method" => "%{[app_log][req][method]}" }
          add_field => { "http_url" => "%{[app_log][req][url]}" }
          add_field => { "http_user_agent" => "%{[app_log][req][headers][user-agent]}" }
        }

        if [app_log][req][id] {
          mutate {
            add_field => { "request_id" => "%{[app_log][req][id]}" }
          }
        }
      }

      if [app_log][res] {
        mutate {
          add_field => { "http_status_code" => "%{[app_log][res][statusCode]}" }
        }

        convert => { "http_status_code" => "integer" }
      }

      # Extract performance metrics
      if [app_log][responseTime] {
        mutate {
          add_field => { "response_time" => "%{[app_log][responseTime]}" }
        }
        convert => { "response_time" => "float" }
      }

      if [app_log][duration] {
        mutate {
          add_field => { "duration" => "%{[app_log][duration]}" }
        }
        convert => { "duration" => "float" }
      }
    } else {
      # Parse unstructured logs with grok patterns
      grok {
        match => {
          "message" => '\[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:log_level} %{DATA:service_name}: %{GREEDYDATA:log_message}'
        }
        tag_on_failure => ["_grokparsefailure_unstructured"]
      }

      if "_grokparsefailure_unstructured" not in [tags] {
        date {
          match => [ "timestamp", "ISO8601" ]
        }
      }
    }
  }

  # Process database logs
  if [service_name] =~ /(mongo|postgres|redis)/ {
    if [service_name] =~ /mongo/ {
      # Parse MongoDB logs
      if [message] =~ /^\{.*\}$/ {
        json {
          source => "message"
          target => "mongo_log"
        }

        if [mongo_log][t][`$date`] {
          date {
            match => [ "[mongo_log][t][`$date`]", "ISO8601" ]
          }
        }

        if [mongo_log][s] {
          mutate {
            add_field => { "log_level" => "%{[mongo_log][s]}" }
          }
        }

        if [mongo_log][c] {
          mutate {
            add_field => { "mongo_component" => "%{[mongo_log][c]}" }
          }
        }

        if [mongo_log][attr][durationMillis] {
          mutate {
            add_field => { "query_duration" => "%{[mongo_log][attr][durationMillis]}" }
          }
          convert => { "query_duration" => "integer" }
        }
      }
    }

    if [service_name] =~ /postgres/ {
      # Parse PostgreSQL logs
      grok {
        match => {
          "message" => '%{TIMESTAMP_ISO8601:timestamp} %{TZ:timezone} \[%{POSINT:pid}\] %{LOGLEVEL:log_level}:  %{GREEDYDATA:postgres_message}'
        }
        tag_on_failure => ["_grokparsefailure_postgres"]
      }

      if "_grokparsefailure_postgres" not in [tags] {
        date {
          match => [ "timestamp", "ISO8601" ]
        }

        # Extract query duration
        if [postgres_message] =~ /duration:/ {
          grok {
            match => { "postgres_message" => "duration: %{NUMBER:query_duration} ms" }
          }
          convert => { "query_duration" => "float" }
        }
      }
    }

    if [service_name] =~ /redis/ {
      # Parse Redis logs
      grok {
        match => {
          "message" => '%{POSINT:pid}:%{WORD:role} %{MONTHDAY} %{MONTH} %{YEAR} %{TIME:timestamp} %{DATA:log_level} %{GREEDYDATA:redis_message}'
        }
        tag_on_failure => ["_grokparsefailure_redis"]
      }

      if "_grokparsefailure_redis" not in [tags] {
        date {
          match => [ "timestamp", "dd MMM yyyy HH:mm:ss.SSS" ]
        }
      }
    }
  }

  # Normalize log levels
  if [log_level] {
    mutate {
      lowercase => [ "log_level" ]
    }

    # Map different log level formats
    if [log_level] in ["debug", "trace"] {
      mutate { replace => { "log_level" => "debug" } }
    } else if [log_level] in ["info", "information"] {
      mutate { replace => { "log_level" => "info" } }
    } else if [log_level] in ["warn", "warning"] {
      mutate { replace => { "log_level" => "warn" } }
    } else if [log_level] in ["error", "err", "fatal"] {
      mutate { replace => { "log_level" => "error" } }
    }
  }

  # Add GeoIP information for client IPs
  if [client_ip] and [client_ip] != "127.0.0.1" and [client_ip] != "::1" {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }

  # Clean up temporary fields
  mutate {
    remove_field => [ "json_log", "app_log", "mongo_log" ]
  }

  # Add environment and service tags
  mutate {
    add_field => { "environment" => "production" }
    add_tag => [ "klikkflow" ]
  }

  if [service_name] {
    mutate {
      add_tag => [ "%{service_name}" ]
    }
  }

  # Add alert conditions
  if [log_level] == "error" {
    mutate {
      add_tag => [ "alert", "error" ]
    }
  }

  if [http_status_code] and [http_status_code] >= 500 {
    mutate {
      add_tag => [ "alert", "http_error" ]
    }
  }

  if [response_time] and [response_time] > 5000 {
    mutate {
      add_tag => [ "alert", "slow_response" ]
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "elastic"
    password => "elastic123"

    # Dynamic index based on service and date
    index => "%{[@metadata][index]}"

    # Document type
    document_type => "%{[@metadata][document_type]}"

    # Template for index settings
    template_name => "klikkflow-logs"
    template_pattern => "klikkflow-logs-*"
    template => "/usr/share/logstash/templates/klikkflow-logs.json"
    template_overwrite => true
  }

  # Send alerts to dedicated index
  if "alert" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      user => "elastic"
      password => "elastic123"
      index => "klikkflow-alerts-%{+YYYY.MM.dd}"
      document_type => "alert"
    }
  }

  # Debug output for development
  if [@metadata][debug] {
    stdout {
      codec => rubydebug
    }
  }

  # Send metrics to dead letter queue for monitoring
  if "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      user => "elastic"
      password => "elastic123"
      index => "klikkflow-parse-failures-%{+YYYY.MM.dd}"
      document_type => "parse_failure"
    }
  }
}